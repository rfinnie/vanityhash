#!/usr/bin/env python3

########################################################################
# vanityhash, a hex hash fragment creation tool
# Copyright (C) 2010-2018 Ryan Finnie <ryan@finnie.org>
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.
########################################################################

import os
import sys
import argparse
import hashlib
import struct
import multiprocessing
import queue
import time
import datetime
import codecs
import logging
import io


__version__ = '2.0'


def pretty_number(n, divisor=1000, rollover=1.0, format='{number:0.02f} {prefix}'):
    if divisor == 1024:
        prefixes = ['', 'Ki', 'Mi', 'Gi', 'Ti']
    else:
        prefixes = ['', 'K', 'M', 'G', 'T']
    ppos = 0
    max_ppos = len(prefixes) - 1
    while n >= (divisor * rollover):
        ppos = ppos + 1
        n = n / float(divisor)
        if ppos >= max_ppos:
            break
    return format.format(number=n, prefix=prefixes[ppos])


def strip_microseconds(td):
    return td - datetime.timedelta(microseconds=td.microseconds)


def bytes_to_hex(b):
    """Return a hex representation of binary byte data."""
    return codecs.encode(b, 'hex_codec').decode('ascii')


class VanityHash:
    """VanityHash class.

    Child subprocesses will have access to the instance of this class,
    forked from the master.
    """
    # multiprocessing queue object
    queue = multiprocessing.Queue()
    # hashlib context
    ctx = None
    # Total number of hashes searched, across all children
    total_searched = 0
    # Total number of work-seconds done by workers
    total_time_searched = 0.0
    # Total number of hashes found, across all children
    total_found = 0
    # In append mode, whether the binary result has been printed yet
    printed_append = False
    # Start time
    start_time = None
    # Next time to display the progress
    next_progress_time = None
    # Number of real workers to be used
    workers_real = 0
    # List of zero-indexed workers to be used
    workers_real_l = None
    # Total workers in the worker set
    workers_total = 0
    # Proporition of real to total workers
    workers_real_fraction = 0.0
    # Pack type of hash candidates
    pack_type = b'=L'
    # Dict of args for digest() / hexdigest()
    digest_args = None
    # Dict of kwargs for digest() / hexdigest()
    digest_kwargs = None
    # Whether vanityhash is in the middle of shutting down
    in_shutdown = False
    # Time object in use
    clock = time.time

    # Argparse object
    args = None

    def __init__(self):
        self.workers_real_l = []
        self.digest_args = []
        self.digest_kwargs = {}

    def main(self):
        """Main program loop."""
        # Parse getopts.
        self.args = self.parse_args()

        # Logging setup
        if self.args.debug:
            logging_level = logging.DEBUG
        elif self.args.quiet:
            logging_level = logging.ERROR
        else:
            logging_level = logging.INFO
        logging.basicConfig(
            format='%(message)s',
            level=logging_level
        )

        if self.args.list_digests:
            for digest in sorted(set(
                [x.lower() for x in ExtendedHashlib().algorithms_available],
            )):
                print(digest)
            return

        # Figure out the most appropriate time counter
        for mode in ('perf_counter', 'monotonic', 'clock'):
            try:
                info = time.get_clock_info(mode)
            except ValueError:
                continue
            except AttributeError:
                # Python 3.2 compatibility
                self.clock = time.clock
                self.clock()
                break
            if not info.monotonic:
                continue
            if info.resolution > 1e-06:
                continue
            self.clock = getattr(time, mode)
            self.clock()
            break

        self.ctx = ExtendedHashlib().new(self.args.digest_type)
        # Pre-compute the largest integer to be tested.
        self.space_max = 0
        for i in range(0, self.args.bits):
            self.space_max += 2 ** i

        # Build a pack type based on the bits_pack size.
        if self.args.byte_order == 'little':
            self.pack_type = b'<'
        elif self.args.byte_order == 'big':
            self.pack_type = b'>'
        else:
            self.pack_type = b'='
        if self.args.bits_pack == 64:
            self.pack_type += b'Q'
        elif self.args.bits_pack == 32:
            self.pack_type += b'L'
        elif self.args.bits_pack == 16:
            self.pack_type += b'H'
        else:
            self.pack_type += b'B'

        # Read stdin data.
        self.read_data()

        if self.args.find_any_pos:
            logging.info('Searching for {} at any position in a {}-bit space.'.format(
                self.args.find, self.args.bits,
            ))
        else:
            logging.info('Searching for {} at position {} in a {}-bit space.'.format(
                self.args.find, self.args.find_pos, self.args.bits,
            ))

        self.start_time = datetime.datetime.now()

        # Spawn worker children.
        for i in self.workers_real_l:
            worker_name = 'Worker {}'.format(i + 1)
            p = multiprocessing.Process(target=self.worker, args=(worker_name, i,))
            p.name = worker_name
            p.start()

        if self.workers_total == self.workers_real:
            logging.info('Spawned {} worker{}.'.format(
                self.workers_real, ((self.workers_real != 1) and 's' or ''),
            ))
        else:
            logging.info('Spawned {} of {} worker{} ({}).'.format(
                self.workers_real,
                self.workers_total,
                ((self.workers_total != 1) and 's' or ''),
                (','.join(str(x + 1) for x in self.workers_real_l)),
            ))

        # Loop through messages from children, occasionally reporting
        # hashing progress.
        self.next_progress_time = self.start_time + datetime.timedelta(seconds=self.args.progress_interval)
        while True:
            try:
                self.process_message()
                self.report_progress()
            except KeyboardInterrupt:
                logging.info('Stopping workers...')
                self.kill_children()

            if self.args.deadline and (datetime.datetime.now() - self.start_time).total_seconds() >= self.args.deadline:
                self.kill_children()

            if len(multiprocessing.active_children()) == 0:
                break

        if self.args.append and self.args.append_empty and not self.printed_append:
            self.args.write_file.write(b'\x00' * int(self.args.bits_pack / 8))
            self.args.write_file.flush()
            self.printed_append = True

        # Final statistics.
        now = datetime.datetime.now()
        elapsed = now - self.start_time
        hashes_per_sec = self.total_searched / (self.total_time_searched / self.workers_real)
        logging.info('Search finished in {}, {} match{} found in {:0.02f}% of a {}-bit space, {}.'.format(
            elapsed,
            self.total_found,
            ((self.total_found != 1) and 'es' or ''),
            ((self.total_searched - 1) / self.space_max * 100),
            self.args.bits,
            pretty_number(hashes_per_sec, format='{number:0.02f} {prefix}hash/s'),
        ))

    def worker(self, worker_name, num_begin):
        """Process hash instructions in a subprocess.

        Note that the state of the class instance is the state at the
        time of the fork from the parent process.  Communication back to
        the parent is done by the queue object.
        """
        to_find = self.args.find
        to_find_len = len(to_find)
        find_pos = self.args.find_pos
        find_anypos = self.args.find_any_pos
        find_pos_end = find_pos + to_find_len

        # Start out with a group of 10,000 hashes.  This will be revised
        # to be approximately 2 seconds worth of hashes.
        last_report = self.clock()
        report_i = 10000
        i = num_begin

        while i <= self.space_max:
            # Take into account multiple workers when determining when
            # to end the group.
            group_num_end = i + (report_i * self.workers_total)
            if group_num_end > self.space_max:
                group_num_end = self.space_max
            group_i_begin = i

            # The actual hash->test loop is as tight as possible, and
            # hence is duplicated a bit.
            if find_anypos:
                while i <= group_num_end:
                    ctxcopy = self.ctx.copy()
                    ctxcopy.update(struct.pack(self.pack_type, i))
                    hexdigest = ctxcopy.hexdigest(*self.digest_args, **self.digest_kwargs)
                    if hexdigest.find(to_find) > -1:
                        self.queue.put(('FOUND', worker_name, (hexdigest, i)))
                    i += self.workers_total
            else:
                while i <= group_num_end:
                    ctxcopy = self.ctx.copy()
                    ctxcopy.update(struct.pack(self.pack_type, i))
                    hexdigest = ctxcopy.hexdigest(*self.digest_args, **self.digest_kwargs)
                    if hexdigest[find_pos:find_pos_end] == to_find:
                        self.queue.put(('FOUND', worker_name, (hexdigest, i)))
                    i += self.workers_total

            now = self.clock()

            # Figure out how many hashes were performed, and update the
            # parent.
            report_i = int((i - group_i_begin) / self.workers_total)
            self.queue.put(('PROGRESS', worker_name, report_i, (now - last_report)))

            # Figure out how many hashes are needed to run for the next
            # ~2 seconds.
            next_report_i = int(2 * (report_i / (now - last_report)))
            last_report = now
            report_i = next_report_i

    def parse_args(self, argv=None):
        """Parse user arguments."""
        if argv is None:
            argv = sys.argv

        parser = argparse.ArgumentParser(
            description='vanityhash ({})'.format(__version__),
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            prog=os.path.basename(argv[0]),
        )

        parser.add_argument(
            '--version', '-V', action='version',
            version=__version__,
            help='report the program version',
        )

        parser.add_argument(
            'find', type=str, default=None, nargs='?',
            help='Hex string to search for',
        )

        parser.add_argument(
            '--bits', '-b', type=int, default=32,
            help='Search space, in bits',
        )
        parser.add_argument(
            '--workers', '-w', dest='workers_s', type=str, default='guess',
            help='Worker specification',
        )
        parser.add_argument(
            '--digest', '-d', dest='digest_type', type=str, default='md5',
            help='Hash digest type',
        )
        parser.add_argument(
            '--progress', '-s', dest='progress_interval', type=float, default=5.0,
            help='How often to display progress information, in seconds',
        )
        parser.add_argument(
            '--append', '-a', action='store_true',
            help='Whether to output the original data + the first result',
        )
        parser.add_argument(
            '--bits-pack', '-t', type=int, default=0,
            help='Total size containing the search space, in bits',
        )
        parser.add_argument(
            '--any_position', '-y', dest='find_any_pos', action='store_true',
            help='Whether to find the desired fragment anywhere in the hash',
        )
        parser.add_argument(
            '--quiet', '-q', action='store_true',
            help='Whether to display human-readable information to stderr',
        )
        parser.add_argument(
            '--debug', action='store_true',
            help='Print extra debugging information.',
        )
        parser.add_argument(
            '--position', '-p', dest='find_pos', type=int, default=0,
            help='Zero-indexed position within the hash to search',
        )
        parser.add_argument(
            '--byte-order', '-n', choices=['native', 'little', 'big'], default='native',
            help='Endianness of the built container',
        )
        parser.add_argument(
            '--append-empty', '-e', action='store_true',
            help='Where to add a zeroed pack in append mode, if no match is found',
        )
        parser.add_argument(
            '--benchmark', action='store_true',
            help='Turn on options for "standard" benchmark',
        )
        parser.add_argument(
            '--read-file', type=argparse.FileType('rb'), default='-',
            help='File to read',
        )
        parser.add_argument(
            '--write-file', type=argparse.FileType('wb'), default='-',
            help='File to write',
        )
        parser.add_argument(
            '--list-digests', action='store_true',
            help='List available digests',
        )
        parser.add_argument(
            '--deadline', type=float, default=0.0,
            help='Maximum processing time (seconds)',
        )

        args = parser.parse_args(args=argv[1:])

        if args.find == 'benchmark':
            args.bits = 64
            args.digest = 'md5'
            args.deadline = 60.0
            args.find = 'f00fc7c8'
            args.read_file = io.BytesIO(b'foo\n')

        if hasattr(args.read_file, 'buffer'):
            args.read_file = args.read_file.buffer

        if hasattr(args.write_file, 'buffer'):
            args.write_file = args.write_file.buffer

        if '=' in args.digest_type:
            (args.digest_type, digest_len) = args.digest_type.split('=')
            self.digest_args = [int(digest_len)]

        if args.digest_type == 'sha1alt':
            args.digest_type = 'sha1'
        if (args.bits < 1) or (args.bits > 64):
            parser.error('Search space must be 64 bits or less')

        # Generate the container size if not specified.
        if args.bits_pack == 0:
            args.bits_pack = 1
            while args.bits_pack < args.bits:
                args.bits_pack *= 2
            if args.bits_pack < 8:
                args.bits_pack = 8
        # Validate the container size.
        if (args.bits_pack < args.bits) or (args.bits_pack > 64):
            parser.error('Invalid bits-pack')
        # Make sure the container size is a power of 2.
        bits_pack_bytes = int(args.bits_pack / 8)
        if not (bits_pack_bytes & (bits_pack_bytes - 1)) == 0:
            parser.error('Invalid bits-pack')

        # Validate the desired hex fragment
        if args.find is None:
            if not args.list_digests:
                parser.error('Hex string required')
            # Fake value, won't be using it for list_digests
            args.find = 'ffffffff'
        for i in args.find:
            if i not in '0 1 2 3 4 5 6 7 8 9 a b c d e f'.split():
                parser.error('Invalid search hex string')

        # Build the worker options.
        if args.workers_s == 'guess':
            try:
                args.workers_s = str(len(os.sched_getaffinity(0)))
            except AttributeError:
                pass
        if args.workers_s == 'guess':
            try:
                args.workers_s = str(multiprocessing.cpu_count())
            except NotImplementedError:
                args.workers_s = str(1)
        if args.workers_s.isdigit():
            # If a single number is given, the real and total workers
            # are the same.
            self.workers_total = int(args.workers_s)
            self.workers_real_l = range(self.workers_total)
        else:
            # If a specification is given, validate and build according
            # to the specification.
            try:
                (workert, workerx) = args.workers_s.split(':')
            except ValueError:
                parser.error('Invalid worker specification')
            self.workers_total = int(workert)
            for i in workerx.split(','):
                if not i.isdigit():
                    parser.error('Invalid worker specification')
                i = int(i)
                if (i > self.workers_total) or (i < 1):
                    parser.error('Invalid worker specification')
                if not (i - 1) in self.workers_real_l:
                    self.workers_real_l.append(i - 1)
                self.workers_real_l.sort()
        self.workers_real = len(self.workers_real_l)
        if (self.workers_total < 1) or (self.workers_real < 1):
            parser.error('Invalid number of workers')
        self.workers_real_fraction = self.workers_real / self.workers_total

        # Test the hash type is valid.
        try:
            testctx = ExtendedHashlib().new(args.digest_type)
        except ValueError:
            parser.error('Invalid digest type')

        # Test the position specified is correct according to the given
        # hash type.
        hexdigestsize = len(testctx.hexdigest(*self.digest_args, **self.digest_kwargs))
        maxpos = hexdigestsize - len(args.find)
        if args.find_pos < 0:
            args.find_pos += hexdigestsize
        if args.find_pos > maxpos:
            parser.error('Pattern position {} goes beyond end of {} digest'.format(
                args.find_pos, args.digest_type.upper()),
            )

        return args

    def process_message(self):
        """Parse a received child message."""
        try:
            msg = self.queue.get(block=True, timeout=1.0)
        except queue.Empty:
            return
        if msg[0] == 'PROGRESS':
            self.total_searched += msg[2]
            self.total_time_searched += msg[3]
            logging.debug('{} reported {} searched in {:0.04f} seconds'.format(
                msg[1],
                msg[2],
                msg[3],
            ))
        elif msg[0] == 'FOUND':
            (msgdigest, msgdata) = msg[2]
            msgdata = struct.pack(self.pack_type, msgdata)
            logging.info('Match found: 0x{} -> {} {}'.format(
                bytes_to_hex(msgdata), self.args.digest_type.upper(), msgdigest),
            )
            self.total_found += 1
            if self.args.append:
                if not self.printed_append:
                    self.args.write_file.write(msgdata)
                    self.args.write_file.flush()
                    self.printed_append = True
                    self.kill_children()
            else:
                sys.stdout.write('{} {}\n'.format(bytes_to_hex(msgdata), msgdigest))
                sys.stdout.flush()

    def read_data(self):
        """Read data from stdin and build the initial hash context."""
        self.ctx = ExtendedHashlib().new(self.args.digest_type)
        logging.info('Reading input data and adding to digest...')
        datalen = 0
        while True:
            buf = self.args.read_file.read(1024)
            if not buf:
                break
            if self.args.append:
                self.args.write_file.write(buf)
            datalen += len(buf)
            self.ctx.update(buf)
        if self.args.append:
            self.args.write_file.flush()
        logging.info('Done.')

        origdigest = self.ctx.copy().hexdigest(*self.digest_args, **self.digest_kwargs)
        logging.info('Original data: {} bytes, {} {}'.format(datalen, self.args.digest_type.upper(), origdigest))

    def report_progress(self):
        """Occasionally output progress statistics."""
        now = datetime.datetime.now()
        if not now > self.next_progress_time:
            return
        elapsed = now - self.start_time
        percent = self.total_searched / (self.space_max * self.workers_real_fraction) * 100
        if self.total_searched > 0:
            hashes_per_sec = self.total_searched / (self.total_time_searched / self.workers_real)
            try:
                estimated_time = strip_microseconds(datetime.timedelta(
                    seconds=((self.space_max * self.workers_real_fraction) / hashes_per_sec),
                ))
                remaining = strip_microseconds(datetime.timedelta(
                    seconds=((self.space_max * self.workers_real_fraction - self.total_searched) / hashes_per_sec),
                ))
                if remaining > datetime.timedelta(days=365):
                    logging.info('{:0.02f}% searched in {} of ~{}, {}'.format(
                        percent,
                        strip_microseconds(elapsed),
                        estimated_time,
                        pretty_number(hashes_per_sec, format='{number:0.02f} {prefix}hash/s'),
                    ))
                else:
                    logging.info('{:0.02f}% searched in {}, ~{} remaining of ~{}, {}'.format(
                        percent,
                        strip_microseconds(elapsed),
                        remaining,
                        estimated_time,
                        pretty_number(hashes_per_sec, format='{number:0.02f} {prefix}hash/s'),
                    ))
            except OverflowError:
                logging.info('{:0.02f}% searched in {}, {}'.format(
                    percent,
                    strip_microseconds(elapsed),
                    pretty_number(hashes_per_sec, format='{number:0.02f} {prefix}hash/s'),
                ))
        else:
            logging.info('{:0.02f}% searched in {}'.format(
                strip_microseconds(percent),
                elapsed,
            ))
        self.next_progress_time = now + datetime.timedelta(seconds=self.args.progress_interval)

    def kill_children(self):
        """Kill all child subprocesses."""
        if self.in_shutdown:
            return
        self.in_shutdown = True
        for child_process in multiprocessing.active_children():
            child_process.terminate()


class ExtendedHashlib:
    """hashlib-compatible extension system."""
    def __init__(self):
        self.algorithms_available = set()
        for name in hashlib.algorithms_available:
            try:
                vars(self)[name] = getattr(hashlib, name)
                self.algorithms_available.update([name])
            except AttributeError:
                pass

        try:
            import hashlib_additional
            for name in hashlib_additional.algorithms_available:
                try:
                    vars(self)[name] = getattr(hashlib_additional, name)
                    self.algorithms_available.update([name])
                except AttributeError:
                    pass
        except ImportError:
            pass

        # https://github.com/rfinnie/vanityhash/issues/1
        try:
            import xxhash
            for name in ('xxh32', 'xxh64'):
                try:
                    vars(self)[name] = getattr(xxhash, name)
                    self.algorithms_available.update([name])
                except AttributeError:
                    pass
        except ImportError:
            pass

        self.algorithms_guaranteed = hashlib.algorithms_guaranteed

    def new(self, name, *args, **kwargs):
        return getattr(self, name)(*args, **kwargs)


if __name__ == '__main__':
    vh = VanityHash()
    vh.main()
